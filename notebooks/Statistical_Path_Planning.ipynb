{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "18f38cd8-2db7-4cd1-8a88-3296cd815a45",
      "metadata": {
        "id": "18f38cd8-2db7-4cd1-8a88-3296cd815a45"
      },
      "source": [
        "# Statistical Path Planning"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8ec1aa74-49ea-4577-86d5-fdcac44fa885",
      "metadata": {
        "id": "8ec1aa74-49ea-4577-86d5-fdcac44fa885"
      },
      "source": [
        "## Introduction"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9c657776-2307-4bee-a089-876c5e2fab88",
      "metadata": {
        "id": "9c657776-2307-4bee-a089-876c5e2fab88"
      },
      "source": [
        "Multi-agent path planning (MAPP) is the task of finding efficient, collision-free paths for multiple agents within a shared environment. This problem arises in various fields, from robotics and autonomous vehicles to video games and disaster response. One common approach to solving MAPP is conflict-based search (CBS), which is an optimal algorithm that divides the planning problem into smaller subproblems and solves them recursively.\n",
        "\n",
        "However, CBS requires complete knowledge of the environment prior to planning, which can be problematic in dynamic or unpredictable scenarios. To address this limitation, we train a lightweight statistical model to mimic the behavior of CBS by predicting the actions agents will take given their local field of views (FOVs). Unlike traditioanl CBS, our approach is decentralized and requires only partial knowledge of the environment. \n",
        "\n",
        "In this notebook, we will walk through the implementation and evaluation of our approach. We start by loading the CBS-5 dataset, containing millions of CBS actions made within different contexts. We then train and evaluate three models on the dataset, including a logistic regression model, a multi-layer perceptron (MLP), and a small convolutional neural network (CNN)."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dependencies"
      ],
      "metadata": {
        "id": "7Ta4sFn4ZawE"
      },
      "id": "7Ta4sFn4ZawE"
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q tensorflow keras scikit-learn pillow"
      ],
      "metadata": {
        "id": "VN6poI1VZd0s"
      },
      "id": "VN6poI1VZd0s",
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "91782aaf-ed10-4f45-8dde-0cd1a58aa919",
      "metadata": {
        "id": "91782aaf-ed10-4f45-8dde-0cd1a58aa919"
      },
      "source": [
        "## Dataset\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "019c27fc-aad8-472e-a0ad-547d67f4fc9c",
      "metadata": {
        "id": "019c27fc-aad8-472e-a0ad-547d67f4fc9c"
      },
      "source": [
        "The CBS-5 dataset contains over 5 million actions taken by CBS agents in a variety of environments and scenarios. Each example maps an agent's local field of view (FOV) to one of five actions (`Stay`, `North`, `East`, `South`, `West` and). The FOV is a 7x7 binary image with three channels:\n",
        "\n",
        "- `State`: encodes the relative position of agents within the FOV.\n",
        "- `Goal`: encodes the relative position of an agent's goal within the FOV. Clipped to the FOV boundary when out of range.\n",
        "- `Map`: encodes the relative position of obstacles within the FOV.\n",
        "\n",
        "The dataset is stored in a bespoke compressed format, so we'll first write a small function to decompress it:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "232ba8df-ee7d-46a8-940a-549ec467af16",
      "metadata": {
        "jupyter": {
          "source_hidden": true
        },
        "tags": [],
        "id": "232ba8df-ee7d-46a8-940a-549ec467af16"
      },
      "outputs": [],
      "source": [
        "import gzip\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def load_chunk(path: str, shape: tuple = tuple()) -> np.array:\n",
        "    \"\"\"\n",
        "    Decompresses a CBS-5 chunk and loads it into memory\n",
        "    as a NumPy array.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "\n",
        "    path: str\n",
        "\n",
        "        The location of CBS-5 chunk.\n",
        "\n",
        "    shape: tuple = (,)\n",
        "    \n",
        "        The shape of the chunk array.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "\n",
        "    chunk: np.array\n",
        "\n",
        "        The decompressed chunk.\n",
        "    \"\"\"\n",
        "    \n",
        "    with gzip.GzipFile(path) as file:\n",
        "        \n",
        "        chunk = np.array(list(file.read()))        \n",
        "        chunk = chunk.reshape((-1, *shape))\n",
        "        chunk = chunk.astype(float)\n",
        "        \n",
        "        return chunk"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "385b7ef6-1578-4652-8a68-5ea8d77a3b69",
      "metadata": {
        "id": "385b7ef6-1578-4652-8a68-5ea8d77a3b69"
      },
      "source": [
        "Now let's download and decompress the two chunks from CBS-5 to use as our train and test datasets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "e610b95c-c5d1-42dc-b97b-fca5e0d86ac5",
      "metadata": {
        "jupyter": {
          "source_hidden": true
        },
        "tags": [],
        "id": "e610b95c-c5d1-42dc-b97b-fca5e0d86ac5"
      },
      "outputs": [],
      "source": [
        "!git clone -q https://github.com/oelin/cbs-5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "69fbe8ad-2095-42ac-a01e-fcc56795b59d",
      "metadata": {
        "jupyter": {
          "source_hidden": true
        },
        "tags": [],
        "id": "69fbe8ad-2095-42ac-a01e-fcc56795b59d"
      },
      "outputs": [],
      "source": [
        "labels_train = load_chunk('./cbs-5/v0/labels/labels0.gz')\n",
        "inputs_train = load_chunk('./cbs-5/v0/inputs/inputs0.gz', shape=(7, 7, 3))\n",
        "labels_test = load_chunk('./cbs-5/v1/labels/labels0.gz')\n",
        "inputs_test = load_chunk('./cbs-5/v1/inputs/inputs0.gz', shape=(7, 7, 3))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bb1b22ec-3cd5-4e5e-9577-cc0b48b4dad5",
      "metadata": {
        "id": "bb1b22ec-3cd5-4e5e-9577-cc0b48b4dad5"
      },
      "source": [
        "Let's see how many examples are in each chunk."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "2c1c255a-fe77-4964-a5a1-d48c10063f13",
      "metadata": {
        "jupyter": {
          "source_hidden": true
        },
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2c1c255a-fe77-4964-a5a1-d48c10063f13",
        "outputId": "853ebcfa-d31c-4e7c-f213-9b9355ba22be"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((1130725,), (1131310,))"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "labels_train.shape, labels_test.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bf091d26-398f-4ec1-a848-086496649791",
      "metadata": {
        "id": "bf091d26-398f-4ec1-a848-086496649791"
      },
      "source": [
        "Nice, there's roughly 1.1 million examples per chunk. Not only do we have have a large number of examples, but the train-test split it ~50%. This will futher mitigate risks of overfitting during training."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "15a81d6b-c58d-47f5-b51c-3bda2fe7e439",
      "metadata": {
        "id": "15a81d6b-c58d-47f5-b51c-3bda2fe7e439"
      },
      "source": [
        "Let's take a look at a single example from the train dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "8b1cde6f-2f3d-4876-9ee0-e8eabc9341d6",
      "metadata": {
        "jupyter": {
          "source_hidden": true
        },
        "tags": [],
        "id": "8b1cde6f-2f3d-4876-9ee0-e8eabc9341d6"
      },
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "\n",
        "\n",
        "def show_fov_channel(channel: np.array) -> Image:\n",
        "    \"\"\"\n",
        "    Displays a FOV channel as an image.\n",
        "    \"\"\"\n",
        "    \n",
        "    channel = channel * 255\n",
        "    channel = channel.astype('uint8')\n",
        "    channel = channel.repeat(20, 0).repeat(20, 1)\n",
        "    image = Image.fromarray(channel)\n",
        "    \n",
        "    return image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "66117797-4f3e-43cc-b201-b435783f0c3a",
      "metadata": {
        "jupyter": {
          "source_hidden": true
        },
        "tags": [],
        "id": "66117797-4f3e-43cc-b201-b435783f0c3a"
      },
      "outputs": [],
      "source": [
        "# Separate FOV channels (State, Goal, Map).\n",
        "\n",
        "example = inputs_train[0]\n",
        "example_state = example[:, :, 2]\n",
        "example_goal = example[:, :, 1]\n",
        "example_map = example[:, :, 0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "ec2fefdf-8b0e-4f83-84ba-042b287bd562",
      "metadata": {
        "jupyter": {
          "source_hidden": true
        },
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 174
        },
        "id": "ec2fefdf-8b0e-4f83-84ba-042b287bd562",
        "outputId": "0f469ff6-c72a-4c2e-9f73-8d54f6dade50"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "State:\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<PIL.Image.Image image mode=L size=140x140 at 0x7FDB07D05C70>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAIwAAACMCAAAAACLqx7iAAAAZElEQVR4nO3YMQqAQAwAwdP//1mLa10Q7gqRmT5h22QMAAAAAAAAAID/OFaGr80Lz4XZ7cQUMUVMEVPEFDFFTBFTxAAAANPrt+3un++TT10HYoqYIqaIKWKKmCKmiCliAACA6QZyWgJPPDrCewAAAABJRU5ErkJggg==\n"
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "print('State:')\n",
        "show_fov_channel(example_state)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "6e998d9f-7cdc-426c-94bd-671233f42506",
      "metadata": {
        "jupyter": {
          "source_hidden": true
        },
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 174
        },
        "id": "6e998d9f-7cdc-426c-94bd-671233f42506",
        "outputId": "ff3821b6-324b-4bc7-f402-282f5c02426b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Goal:\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<PIL.Image.Image image mode=L size=140x140 at 0x7FDA48E22640>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAIwAAACMCAAAAACLqx7iAAAASUlEQVR4nO3OMRIAEBRDwc/970yhlZYZdss0eVUAAAAAAAAAAO9oYzcez1j6pd8tMYmYREwiJhGTiEnEJGISMQAAAAAAAAAAP5hEGgEo/adjLAAAAABJRU5ErkJggg==\n"
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "print('Goal:')\n",
        "show_fov_channel(example_goal)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "d9ddb0bc-3c7f-42fd-b9ad-2832f8eda5bf",
      "metadata": {
        "jupyter": {
          "source_hidden": true
        },
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 174
        },
        "id": "d9ddb0bc-3c7f-42fd-b9ad-2832f8eda5bf",
        "outputId": "d7e88a2c-7e5d-4ed9-d5e0-05fae8ec242a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Map:\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<PIL.Image.Image image mode=L size=140x140 at 0x7FDAE778D550>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAIwAAACMCAAAAACLqx7iAAAAd0lEQVR4nO3awQmAMBQFQRX7b1kPnhciYhCZaeDt9YcsCwAAAAAAAADAf6wzRo7B4e3tkDvEFDFFTBFTxBQxRUwRU8QAAACXdfSNdoZPXQdiipgipogpYoqYIqaIKWLKPmPEX4inxBQxRUwRU8QUMUVMEVM+FXMCD1YDee3B4RcAAAAASUVORK5CYII=\n"
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "print('Map:')\n",
        "show_fov_channel(example_map)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e28e04cb-766b-4763-a7c5-ce25b874df6a",
      "metadata": {
        "id": "e28e04cb-766b-4763-a7c5-ce25b874df6a"
      },
      "source": [
        "In this moment, the agent's target it to the *left* and several obstacles sit to right and bottom left of the agent. We would expect the action taken in this sitution to be `West`. Let's see if we're correct."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "62a7c4ef-347b-4869-ac88-7d91434b0727",
      "metadata": {
        "jupyter": {
          "source_hidden": true
        },
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "62a7c4ef-347b-4869-ac88-7d91434b0727",
        "outputId": "8819ba88-475c-428c-955e-4f5aa4c71f33"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Action: 4.0\n"
          ]
        }
      ],
      "source": [
        "print(f'Action: {labels_train[0]}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cf00527a-c885-4f56-811b-9b44641903b1",
      "metadata": {
        "id": "cf00527a-c885-4f56-811b-9b44641903b1"
      },
      "source": [
        "Action 4 corresponds to `West`!"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "88d5319e-12fc-4417-9777-cd903d81ea82",
      "metadata": {
        "id": "88d5319e-12fc-4417-9777-cd903d81ea82"
      },
      "source": [
        "One last thing we need to do is \"one-hot\" encode the lables, as neural networks cannot handle categorical variables directly. We can use Scikit-learn's `OneHotEncoder` preprocessor class for this."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "6a882679-1590-4fe7-884a-ff6791613908",
      "metadata": {
        "jupyter": {
          "source_hidden": true
        },
        "tags": [],
        "id": "6a882679-1590-4fe7-884a-ff6791613908"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "one_hot_encoder = OneHotEncoder(sparse_output=False)\n",
        "\n",
        "y_train = one_hot_encoder.fit_transform(labels_train.reshape(-1, 1))\n",
        "y_test = one_hot_encoder.fit_transform(labels_test.reshape(-1, 1))\n",
        "\n",
        "X_train, X_test = inputs_train, inputs_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "b006f0d5-8ba8-4099-afa3-8f1d92b1c126",
      "metadata": {
        "jupyter": {
          "source_hidden": true
        },
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b006f0d5-8ba8-4099-afa3-8f1d92b1c126",
        "outputId": "b953e228-0d5a-4c8e-821c-dd21aeba4c32"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0., 0., 0., 0., 1.])"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "y_train[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "05bb7c74-f2c1-4086-a41f-62f6d7d08868",
      "metadata": {
        "id": "05bb7c74-f2c1-4086-a41f-62f6d7d08868"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d7b51afd-49c6-4ab2-842e-622977caa811",
      "metadata": {
        "id": "d7b51afd-49c6-4ab2-842e-622977caa811"
      },
      "source": [
        "Now we're ready to start training some models. As mentioned in the introduction, we'll be training a logistic regression model, an MLP and a CNN. It's often best to start with the simplest model and increase complexity if needed. In this case, logistic regression is the simplest with only 735 parameters (7x7x3x4). We'll Keras's `Sequential` function to construct the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "259db521-0fdc-4818-b725-371ee87c07c6",
      "metadata": {
        "tags": [],
        "id": "259db521-0fdc-4818-b725-371ee87c07c6"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow.keras.backend as K\n",
        "from tensorflow.keras.layers import Dense, Conv2D, Flatten\n",
        "from tensorflow.keras.models import Sequential\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Flatten())\n",
        "model.add(Dense(5, activation='softmax'))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hqI4E9sobYho",
        "outputId": "d7ecd6fa-d72c-4acb-c89a-5d75dd472d46"
      },
      "id": "hqI4E9sobYho",
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " flatten_2 (Flatten)         (None, 147)               0         \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 5)                 740       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 740\n",
            "Trainable params: 740\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's compile and optimize the model."
      ],
      "metadata": {
        "id": "rrwPvi3AaVc9"
      },
      "id": "rrwPvi3AaVc9"
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "8b4c2f09-b582-483f-96d9-9019850eb876",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8b4c2f09-b582-483f-96d9-9019850eb876",
        "outputId": "654ef473-928c-45cb-f0f7-2c44f1d3cdb9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/2\n",
            "35336/35336 [==============================] - 99s 3ms/step - loss: 0.1746 - accuracy: 0.9463 - val_loss: 0.1084 - val_accuracy: 0.9577\n",
            "Epoch 2/2\n",
            "35336/35336 [==============================] - 93s 3ms/step - loss: 0.1073 - accuracy: 0.9580 - val_loss: 0.1069 - val_accuracy: 0.9578\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fd9c9ba73d0>"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Remarkably, after only two epochs, this simple linear model is able to achieve above 95% accuracy. This means that in 95% of contexts, the model is able to predict which action a CBS agent will take. Note that a randomly guessing model would achieve only 20%. Additionally, we see that the model performs equally well (sometimes slightly better) on unseen examples, showing that it has not overfit and is likely to generalize well. "
      ],
      "metadata": {
        "id": "cMq1X9CWbT5A"
      },
      "id": "cMq1X9CWbT5A"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we consider an MLP architecture. This is virutally identical however features additional hidden layers. We choose to use three hidden layers."
      ],
      "metadata": {
        "id": "u6y7QSwIb-bH"
      },
      "id": "u6y7QSwIb-bH"
    },
    {
      "cell_type": "code",
      "source": [
        "model2 = Sequential()\n",
        "model2.add(Flatten())\n",
        "model2.add(Dense(256, activation='relu'))\n",
        "model2.add(Dense(128, activation='relu'))\n",
        "model2.add(Dense(64, activation='relu'))\n",
        "model2.add(Dense(5, activation='softmax'))"
      ],
      "metadata": {
        "id": "9zA_rwb-acct"
      },
      "id": "9zA_rwb-acct",
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model2.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YiusMOS9dGQC",
        "outputId": "4cd0fe7c-3c56-426a-830c-cdd4cad6b257"
      },
      "id": "YiusMOS9dGQC",
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " flatten_3 (Flatten)         (None, 147)               0         \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 256)               37888     \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 128)               32896     \n",
            "                                                                 \n",
            " dense_5 (Dense)             (None, 64)                8256      \n",
            "                                                                 \n",
            " dense_6 (Dense)             (None, 5)                 325       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 79,365\n",
            "Trainable params: 79,365\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model2.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "model2.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cEwigL1gcrwu",
        "outputId": "fb713abd-e3ea-4b55-ae30-dfc91dd246bc"
      },
      "id": "cEwigL1gcrwu",
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/2\n",
            "35336/35336 [==============================] - 165s 5ms/step - loss: 0.0961 - accuracy: 0.9630 - val_loss: 0.0842 - val_accuracy: 0.9674\n",
            "Epoch 2/2\n",
            "35336/35336 [==============================] - 163s 5ms/step - loss: 0.0813 - accuracy: 0.9686 - val_loss: 0.0823 - val_accuracy: 0.9683\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fd9c9a8fa30>"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This model performs slightly better however is considerably larger with 79 thousand parameters instead of 740. In this case, the improvement to performance is probably not worth the additional compute."
      ],
      "metadata": {
        "id": "edhCkv9YeDpt"
      },
      "id": "edhCkv9YeDpt"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, we test a small CNN model."
      ],
      "metadata": {
        "id": "HRSgK-PaeX4Y"
      },
      "id": "HRSgK-PaeX4Y"
    },
    {
      "cell_type": "code",
      "source": [
        "model3 = Sequential()\n",
        "\n",
        "# Feature extraction.\n",
        "\n",
        "model3.add(Conv2D(64, kernel_size=3, activation='relu', input_shape=X_train[0].shape))\n",
        "model3.add(Conv2D(32, kernel_size=3, activation='relu'))\n",
        "model3.add(Flatten())\n",
        "\n",
        "# Prediction.\n",
        "\n",
        "model3.add(Dense(5, activation='softmax'))"
      ],
      "metadata": {
        "id": "_iwk4JmqeW0O"
      },
      "id": "_iwk4JmqeW0O",
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model3.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a6wakKAxesoB",
        "outputId": "1de6472d-96c5-41d1-fefa-d2cd4c887b4d"
      },
      "id": "a6wakKAxesoB",
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_5\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_2 (Conv2D)           (None, 5, 5, 64)          1792      \n",
            "                                                                 \n",
            " conv2d_3 (Conv2D)           (None, 3, 3, 32)          18464     \n",
            "                                                                 \n",
            " flatten_4 (Flatten)         (None, 288)               0         \n",
            "                                                                 \n",
            " dense_8 (Dense)             (None, 5)                 1445      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 21,701\n",
            "Trainable params: 21,701\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model3.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "model3.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=4)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 397
        },
        "id": "AgdOo5MHe2WT",
        "outputId": "2bb71cca-550c-41a9-a8fc-9e3dfbc2069e"
      },
      "id": "AgdOo5MHe2WT",
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/4\n",
            "35336/35336 [==============================] - 197s 6ms/step - loss: 0.0944 - accuracy: 0.9639 - val_loss: 0.0818 - val_accuracy: 0.9684\n",
            "Epoch 2/4\n",
            "35331/35336 [============================>.] - ETA: 0s - loss: 0.0813 - accuracy: 0.9683"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-33-3b0d2993297f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'adam'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'categorical_crossentropy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1692\u001b[0m                             \u001b[0msteps_per_execution\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_steps_per_execution\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1693\u001b[0m                         )\n\u001b[0;32m-> 1694\u001b[0;31m                     val_logs = self.evaluate(\n\u001b[0m\u001b[1;32m   1695\u001b[0m                         \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_x\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1696\u001b[0m                         \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_y\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing, return_dict, **kwargs)\u001b[0m\n\u001b[1;32m   2038\u001b[0m                         ):\n\u001b[1;32m   2039\u001b[0m                             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_test_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2040\u001b[0;31m                             \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2041\u001b[0m                             \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2042\u001b[0m                                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    878\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 880\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    881\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    882\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    917\u001b[0m       \u001b[0;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    918\u001b[0m       \u001b[0;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 919\u001b[0;31m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_creation_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    920\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_created_variables\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mALLOW_DYNAMIC_VARIABLE_CREATION\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    921\u001b[0m         raise ValueError(\"Creating variables on a non-first call to a function\"\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    132\u001b[0m       (concrete_function,\n\u001b[1;32m    133\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m--> 134\u001b[0;31m     return concrete_function._call_flat(\n\u001b[0m\u001b[1;32m    135\u001b[0m         filtered_flat_args, captured_inputs=concrete_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[1;32m    136\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1743\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1744\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1745\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1746\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1747\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    376\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    377\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 378\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    379\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     50\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     53\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     54\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As with the MLP, we see marginal improvement from the baseline logistic regression model, however this improvement comes at the cost of additional overhead. The fact that more complex models do not significantly improve performance suggests there is in a sense *nothing more to learn* from an agent's FOV. To improve predictive accuracy further, we would likely need to consider additional factors, such as inter-agent communication. "
      ],
      "metadata": {
        "id": "KGxAUgTygjzm"
      },
      "id": "KGxAUgTygjzm"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's save all three models."
      ],
      "metadata": {
        "id": "TQEnZfxbhqy6"
      },
      "id": "TQEnZfxbhqy6"
    },
    {
      "cell_type": "code",
      "source": [
        "model.save('statistical_path_planning_logistic_regression.h5')\n",
        "model2.save('statistical_path_planning_mlp.h5')\n",
        "model3.save('statistical_path_planning_cnn.h5')"
      ],
      "metadata": {
        "id": "IRXMAJRJhtCK"
      },
      "id": "IRXMAJRJhtCK",
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Visualizations"
      ],
      "metadata": {
        "id": "9R1AL3z-hb8o"
      },
      "id": "9R1AL3z-hb8o"
    },
    {
      "cell_type": "markdown",
      "source": [
        "To aid in model explainability, we now use a simple weight visualization technique for the 740-parameter logistic regression model."
      ],
      "metadata": {
        "id": "zSRn2Vh2hifA"
      },
      "id": "zSRn2Vh2hifA"
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize(data: np.array) -> np.array:\n",
        "    \"\"\"\n",
        "    Normalize a NumPy array.\n",
        "    \"\"\"\n",
        "\n",
        "    return (data - np.min(data)) / (np.max(data) - np.min(data))\n",
        "\n",
        "\n",
        "def visualize_fov_weights(channel: int = 0, action: int = 0) -> Image:\n",
        "    \"\"\"\n",
        "    Visualizes the weights relating a particular FOV channel to an \n",
        "    agent action.\n",
        "    \"\"\"\n",
        "\n",
        "    weights = model.layers[1].weights[0] # All model weights.\n",
        "    weights = weights.numpy().T[action]\n",
        "    weights = weights.reshape(7, 7, 3).T[channel] \n",
        "    weights = normalize(weights)\n",
        "\n",
        "    # Convert the weights to an image where light pixels indicate a large \n",
        "    # positive weight and dark pixels indicate a large negative weight.\n",
        "\n",
        "    weights = (weights * 255).astype('uint8')\n",
        "    weights = weights.repeat(30, 0).repeat(30, 1)\n",
        "    image = Image.fromarray(weights)\n",
        "\n",
        "    return image"
      ],
      "metadata": {
        "id": "ybjDNqhfe-xN"
      },
      "id": "ybjDNqhfe-xN",
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "visualize_fov_weights(0, 0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 227
        },
        "id": "sRBBX3K_jte-",
        "outputId": "4a9869c1-d8d5-479b-9ab7-0f5a481a0914"
      },
      "id": "sRBBX3K_jte-",
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<PIL.Image.Image image mode=L size=210x210 at 0x7FD9C5629640>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAANIAAADSCAAAAAAa8sGeAAABpElEQVR4nO3csSpGcRyHcYdjYVUyGOUGDNyJ8b0Uk1tgs8lqMFgNekux2khJysSmlDt49uff81m/y3n6jadzprsV8obrPq5nuJ7guo3rJa6ruCqVZFCSQUkGJRmUZFCSQUkGJRmUZFCSQUkGJRmUZFCSwbyD8w+uf7hu4PqA6x6uC1wHvFJJBiUZlGRQkkFJBiUZlGRQkkFJBiUZlGRQkkFJBiUZTKc489uHc1yfcT3A9QJXfqoBr1SSQUkGJRmUZFCSQUkGJRmUZFCSQUkGJRmUZFCSQUkG8zrOR7he4cr/oXrHdYnrDa4DXqkkg5IMSjIoyaAkg5IMSjIoyaAkg5IMSjIoyaAkg5IM5i+cH3F9wfUV13tcN3E9xnXAK5VkUJJBSQYlGZRkUJJBSQYlGZRkUJJBSQYlGZRkUJLBdI3zIa6fuPLXHN+4buF6i+uAVyrJoCSDkgxKMijJoCSDkgxKMijJoCSDkgxKMijJoCSDib+qeML1A9ddXNdwnXH9xXXAK5VkUJJBSQYlGZRkUJJBSQYlGZRkUJJBSQYlGZRkUJLBP/r2G2od0b/lAAAAAElFTkSuQmCC\n"
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "visualize_fov_weights(0, 1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 227
        },
        "id": "Yb--2gF8jvTW",
        "outputId": "717e54f9-1307-4b85-d1da-40bee27ac0d6"
      },
      "id": "Yb--2gF8jvTW",
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<PIL.Image.Image image mode=L size=210x210 at 0x7FD9C9BC2670>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAANIAAADSCAAAAAAa8sGeAAABhElEQVR4nO3dvU4VARhFUX4mGEMpie//StRUWlhaUaARrW1OM6FYk73aEyD73vILM7ffbpa/c72f6/uJ9W6utyd+llSSoCRBSYKSBCUJShKUJChJUJKgJEFJgpIEJQlKEhwf96v3XWN/lvuuse8pF/yWShKUJChJUJKgJEFJgpIEJQlKEpQkKElQkqAkQUmC49Oc9/Xhy1x/z/Vlro9z3S74LZUkKElQkqAkQUmCkgQlCUoSlCQoSVCSoCRBSYKSBMfPOb/N9Wmu3+f6da6vcz3zpClSSYKSBCUJShKUJChJUJKgJEFJgpIEJQlKEpQkKElwfJ7zw1yf57r/I+PHXPdDrHrPha8kQUmCkgQlCUoSlCQoSVCSoCRBSYKSBCUJShJcMOnY74z4M9f9LKlf+w/Pdd8mulz4ShKUJChJUJKgJEFJgpIEJQlKEpQkKElQkqAkwQWTjn0F2LeJM5eL/Vn2hu7/lCQoSVCSoCRBSYKSBCUJShKUJChJUJKgJEFJgpIE/wAWKhZclSE0cwAAAABJRU5ErkJggg==\n"
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "visualize_fov_weights(0, 2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 227
        },
        "id": "A0qWpzOFj_16",
        "outputId": "542fe39d-77c8-4fbd-b5b6-7fe206794bdf"
      },
      "id": "A0qWpzOFj_16",
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<PIL.Image.Image image mode=L size=210x210 at 0x7FD9C56297C0>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAANIAAADSCAAAAAAa8sGeAAABfUlEQVR4nO3dO07YUABE0RBMkwZaJPa/nXTZAF0qOhIlfHYwFK6OdU87kqWrVz7Zvnn+ttzM9ZjrvxNPPrN+nyupJEFJgpIEJQlKEpQkKElQkqAkQUmCkgQlCUoSlCQ4zjS9n1j3rcfWzYWvJEFJgpIEJQlKEpQkKElQkqAkQUmCkgQlCUoSXDDp+Jjz3Vz/zPV+rq9zvZ3r21wveEolCUoSlCQoSVCSoCRBSYKSBCUJShKUJChJUJKgJMFx5r2Jp7n+nuvDXP/OtZsLX0mCkgQlCUoSlCQoSVCSoCRBSYKSBCUJShKUJLhg0nHmy0t7fZzrz7n+mOt+E+SCp1SSoCRBSYKSBCUJShKUJChJUJKgJEFJgpIEJQlKEnxxc/F/ri9z/TXXfTdxxgVPqSRBSYKSBCUJShKUJChJUJKgJEFJgpIEJQlKEpQk+OJH2bt4f0tq303sP1ls+98cFzylkgQlCUoSlCQoSVCSoCRBSYKSBCUJShKUJChJUJLgE0FPFHjB9MgTAAAAAElFTkSuQmCC\n"
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "visualize_fov_weights(0, 3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 227
        },
        "id": "Zw9AEUMBkBuS",
        "outputId": "4303107e-870c-4e25-bad1-d0db06b50c70"
      },
      "id": "Zw9AEUMBkBuS",
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<PIL.Image.Image image mode=L size=210x210 at 0x7FD9C5629310>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAANIAAADSCAAAAAAa8sGeAAABjUlEQVR4nO3dTWoUUQCF0XRsnGQrDlyAy85aYrYQAoIjBc3PDu4gheApvjO90MXHGz6q6/Jw83Fvc/081z//7Lm3B375P1WSoCRBSYKSBCUJShKUJChJUJKgJEFJgpIEJQlKElz3/DLXTwfW17leDqwnPKWSBCUJShKUJChJUJKgJEFJgpIEJQlKEpQkKElQkuD6d877juBprt/m+muuP+e671NOeEolCUoSlCQoSVCSoCRBSYKSBCUJShKUJChJUJKgJMHl+5z3Kxl3c/0x1y9zvT/w3BOeUkmCkgQlCUoSlCQoSVCSoCRBSYKSBCUJShKUJChJcN3/2rTfbtjvXHyd6/Ncf891vydywlMqSVCSoCRBSYKSBCUJShKUJChJUJKgJEFJgpIEJQkuj3Pe38LeX7I48h3tfWPSF7p9JQlKEpQkKElQkqAkQUmCkgQlCUoSlCQoSVCS4IRJ1/0li/1Gxr5BOHL7sJ+71xOeUkmCkgQlCUoSlCQoSVCSoCRBSYKSBCUJShKUJChJ8A5NOSAIcxKbOQAAAABJRU5ErkJggg==\n"
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "visualize_fov_weights(0, 4)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 227
        },
        "id": "8KuJYTD3kDUs",
        "outputId": "b9e73430-04eb-4960-c989-84b91adc2df2"
      },
      "id": "8KuJYTD3kDUs",
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<PIL.Image.Image image mode=L size=210x210 at 0x7FD9C5629E50>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAANIAAADSCAAAAAAa8sGeAAABhElEQVR4nO3dO04jARRE0fFgQUjECtj/noiIkGA0Gf8dlANHp3VPWsLWVYdPbU7Pf5afuf6d62muX1d88vcVf0sqSVCSoCRBSYKSBCUJShKUJChJUJKgJEFJgpIEJQnOn3Pel4ttf/LtXPdtYt9EDviUShKUJChJUJKgJEFJgpIEJQlKEpQkKElQkqAkQUmC8/uc9wVhXx8e5/o01/1GRpcLX0mCkgQlCUoSlCQoSVCSoCRBSYKSBCUJShKUJDhg0ullzndz3ZeLf3N9mOvrXD/mesCnVJKgJEFJgpIEJQlKEpQkKElQkqAkQUmCkgQlCUoSXHjnYv9a1Ntc7+e67xr7e3vnwleSoCRBSYKSBCUJShKUJChJUJKgJEFJgpIEJQkOmHTeTfvthpu5/t9fPNd9ueg/dPtKEpQkKElQkqAkQUmCkgQlCUoSlCQoSVCSoCTBAZP2AeHCBeGaNyP2um8T2wGfUkmCkgQlCUoSlCQoSVCSoCRBSYKSBCUJShKUJChJ8AumDB1RBq8k7QAAAABJRU5ErkJggg==\n"
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "visualize_fov_weights(2, 0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 227
        },
        "id": "xFDrwpVlkE4M",
        "outputId": "eb734145-5649-4f07-e1cc-1599336c09e6"
      },
      "id": "xFDrwpVlkE4M",
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<PIL.Image.Image image mode=L size=210x210 at 0x7FD9C563E250>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAANIAAADSCAAAAAAa8sGeAAABpElEQVR4nO3dsSqHYRxHcY9MFgvKKLkBq0XhJlyBWUa5Azeg7O7AarJJViNlMOifTAa5g7OYztP5rN96306/+e0dZ0vkA9dHXI9x/cX1E9dXXJdxVSrJoCSDkgxKMijJoCSDkgxKMijJoCSDkgxKMijJoCSDwfPRPx59gusbrg+4fuE64ZVKMijJoCSDkgxKMijJoCSDkgxKMijJoCSDkgxKMijJYOzjvI7rLa43uJ7ieoHrD64TXqkkg5IMSjIoyaAkg5IMSjIoyaAkg5IMSjIoyaAkg5IMxjnOu7gucN3E9f4f7/3GdcIrlWRQkkFJBiUZlGRQkkFJBiUZlGRQkkFJBiUZlGRQksHYwvkQ12tc+buJK1wvceW/b0x4pZIMSjIoyaAkg5IMSjIoyaAkg5IMSjIoyaAkg5IMSjIY/B/tA1yfcN3B9RnXDVzXcJ3wSiUZlGRQkkFJBiUZlGRQkkFJBiUZlGRQkkFJBiUZlGSwso3zHa6ruC5wfcf1Bdc9XCe8UkkGJRmUZFCSQUkGJRmUZFCSQUkGJRmUZFCSQUkGJRn8AX2NGpezMtbCAAAAAElFTkSuQmCC\n"
          },
          "metadata": {},
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "visualize_fov_weights(2, 1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 227
        },
        "id": "edVyD2aykIjQ",
        "outputId": "230c26af-918d-431c-ac4b-86052d047722"
      },
      "id": "edVyD2aykIjQ",
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<PIL.Image.Image image mode=L size=210x210 at 0x7FD9C56EE160>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAANIAAADSCAAAAAAa8sGeAAABpUlEQVR4nO3bsSpGcRyHcfQP45vMNqSMb8qgXIDNZLC5DbsrsLgPM1cgdoOUohgYZJE7eBbTc3o+67czPOc3ns7ixQL5xnUT1wdcP3HdwnWG6xKuSiUZlGRQkkFJBiUZlGRQkkFJBiUZlGRQkkFJBiUZlGQwvnBexvUD1wNcr3HlN/30j2eVSjIoyaAkg5IMSjIoyaAkg5IMSjIoyaAkg5IMSjIoyWDw3w3vuJ7jeozrDq4buG7jOsErlWRQkkFJBiUZlGRQkkFJBiUZlGRQkkFJBiUZlGRQksF4xPkW1zNcj3Ddx3UF1zdcJ3ilkgxKMijJoCSDkgxKMijJoCSDkgxKMijJoCSDkgxKMhg/OJ/iyl8u9nC9xPUQ13tcJ3ilkgxKMijJoCSDkgxKMijJoCSDkgxKMijJoCSDkgxKMhjrOD/jeoXrHa4nuL7g+ovrBK9UkkFJBiUZlGRQkkFJBiUZlGRQkkFJBiUZlGRQkkFJBmMV511cb3B9xZW/a6zhOsd1glcqyaAkg5IMSjIoyaAkg5IMSjIoyaAkg5IMSjIoyaAkgz+1Jxn7He2ZnwAAAABJRU5ErkJggg==\n"
          },
          "metadata": {},
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "visualize_fov_weights(2, 2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 227
        },
        "id": "YRCygiI5ktJf",
        "outputId": "03c9db10-6dbc-4595-e52b-4940d2c542ba"
      },
      "id": "YRCygiI5ktJf",
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<PIL.Image.Image image mode=L size=210x210 at 0x7FD9C560B7F0>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAANIAAADSCAAAAAAa8sGeAAABo0lEQVR4nO3bsSqFcRyHcUdvMimSgcWCkkUZrVZX4hZchosxuQPpdAZlUAYdgyxSSriDZ3/+PZ/1u7xPv/HfO7tcIfu47uF6iusS12dcN3FdxVWpJIOSDEoyKMmgJIOSDEoyKMmgJIOSDEoyKMmgJIOSDKYznL9wfcf1AtcbXA9wfcN1wCuVZFCSQUkGJRmUZFCSQUkGJRmUZFCSQUkGJRmUZFCSwTTH+RDXI1wXuH7g+ovrGq4DXqkkg5IMSjIoyaAkg5IMSjIoyaAkg5IMSjIoyaAkg5IMJn4F+MH1FtcXXPl/jVdcP3Ed8EolGZRkUJJBSQYlGZRkUJJBSQYlGZRkUJJBSQYlGZRkMO3gzC8I97g+4vqA6xWuJ7gOeKWSDEoyKMmgJIOSDEoyKMmgJIOSDEoyKMmgJIOSDEoymL5x5vUY1ztcr3HdwHUd1wGvVJJBSQYlGZRkUJJBSQYlGZRkUJJBSQYlGZRkUJJBSQazc5y3cd3F9QnXP1y3cOVvHvBKJRmUZFCSQUkGJRmUZFCSQUkGJRmUZFCSQUkGJRmUZPAPdGYbkiROJgQAAAAASUVORK5CYII=\n"
          },
          "metadata": {},
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "visualize_fov_weights(2, 3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 227
        },
        "id": "gMZ86yfxkylp",
        "outputId": "3f024683-5f70-4e2f-fe6e-21bbf540cb71"
      },
      "id": "gMZ86yfxkylp",
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<PIL.Image.Image image mode=L size=210x210 at 0x7FD9C560B520>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAANIAAADSCAAAAAAa8sGeAAABpklEQVR4nO3cPy5EYRxGYVemUTIbsAgR0VApJLTWYQ1qpcoKrEChpFRMMqHQ2IBmNIL4s4PTny/nad/q3F95c+90sUYOcb3EdYXrOa43uM5xXcdVqSSDkgxKMijJoCSDkgxKMijJoCSDkgxKMijJoCSDkgymJc6buF7juo3rC647uD7jOuCVSjIoyaAkg5IMSjIoyaAkg5IMSjIoyaAkg5IMSjIoyWDiNwg/uL7heoXrA677uJ7iOuCVSjIoyaAkg5IMSjIoyaAkg5IMSjIoyaAkg5IMSjIoyWD6w/kbV36vsYHrEa6fuL7jOuCVSjIoyaAkg5IMSjIoyaAkg5IMSjIoyaAkg5IMSjIoyWD2gfMdrvw8TnB9xXUX1zNcB7xSSQYlGZRkUJJBSQYlGZRkUJJBSQYlGZRkUJJBSQYlGUxPOC9w5S8yHnHdwvUX1y9cB7xSSQYlGZRkUJJBSQYlGZRkUJJBSQYlGZRkUJJBSQYlGczmOPP/oI5xvcd1D9dbXA9wHfBKJRmUZFCSQUkGJRmUZFCSQUkGJRmUZFCSQUkGJRmUZPAPXfoemK3BrGkAAAAASUVORK5CYII=\n"
          },
          "metadata": {},
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "visualize_fov_weights(2, 4)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 227
        },
        "id": "pxFUTVfhk0Sp",
        "outputId": "87c47525-2578-4ec3-c579-ed519affbc60"
      },
      "id": "pxFUTVfhk0Sp",
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<PIL.Image.Image image mode=L size=210x210 at 0x7FD9C560BC40>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAANIAAADSCAAAAAAa8sGeAAABpElEQVR4nO3dPS4GYRhGYcMkRFSEhE6nUX21dWhsQmURdmAF9qC0B0vQfEgQEsTPDk6jOm/O1d7F5OQpJ5OZblbIG653uG7j+o3rPq6buK7iqlSSQUkGJRmUZFCSQUkGJRmUZFCSQUkGJRmUZFCSQUkG8yfOj7ge4nqB6yWuz7iu4zrglUoyKMmgJIOSDEoyKMmgJIOSDEoyKMmgJIOSDEoyKMlgfsd5D9dzXM9wPcX1CtcHXAe8UkkGJRmUZFCSQUkGJRmUZFCSQUkGJRmUZFCSQUkGJRlM1zgvcd3F9QTXW1w/cJ1wHfBKJRmUZFCSQUkGJRmUZFCSQUkGJRmUZFCSQUkGJRmUZDD/4MzvJp5wPcD1HtcjXF9wHfBKJRmUZFCSQUkGJRmUZFCSQUkGJRmUZFCSQUkGJRmUZDDv4LyG6zGuv7gucOVvPfjv3gNeqSSDkgxKMijJoCSDkgxKMijJoCSDkgxKMijJoCSDkgzmL5xfceXvNTZw5f9rbP3juQNeqSSDkgxKMijJoCSDkgxKMijJoCSDkgxKMijJoCSDkgz+AI2CHSuhaNyiAAAAAElFTkSuQmCC\n"
          },
          "metadata": {},
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3ZlUqZCRk1ep"
      },
      "id": "3ZlUqZCRk1ep",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.2"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}